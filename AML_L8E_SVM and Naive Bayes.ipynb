{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import necessary libraries and specify that graphs should be plotted inline. \n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data for Scikit-Learn Datasets\n",
    "In today's practice, we will use two datasets: the cancer dataset, and the iris dataset. Both are sklearn-embedded datasets. Run cell below to check details for cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Elements dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer # Loading all info of cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "print(\"Key Elements\", cancer.keys())\n",
    "# cancer.target_names\n",
    "# cancer.feature_names\n",
    "# print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on whether the separator is linear (i.e., using kernel function or not), we classify the SVM approach into two types: linear SVM and kernel SVM. These two approaches are realized through different syntax in Scikit-Learn. \n",
    "\n",
    "Recall that we also mentioned hard/soft-SVM, based on whether the classifier allows for noisy data points. This difference will be accommodated using different values of hyperparameter C. We set C to a very large number as an approximation of hard-margin SVM.\n",
    "\n",
    "### Linear SVM\n",
    "For a baisc Linear SVM classifier, we use syntax:\n",
    "**<center>sklearn.svm.LinearSVC()</center>**\n",
    "- **C:** Hyperparameter of how acceptable the model is for margin violations. Smaller C indicates more acceptability. Default value is 1.\n",
    "- Set random state for technical reasons.\n",
    "\n",
    "#### Practice\n",
    "- Load cancer data (sklearn.datasets.load_breast_cancer), use all variables (except the target) as predictors, split the data.\n",
    "- Train a linear SVC, leave all settings as default. \n",
    "    - What is the training and test score?\n",
    "- Train a linear SVC with grid search and 5-fold cross validation. *(Hint: You may want to set n_jobs = 2 as an input parameter for GridSearchCV. This saves some time for computation)*\n",
    "    - Let choices of C be: [0.001, 0.01, 0.1, 1, 10, 100, 100000]. \n",
    "    - What is the the best C? \n",
    "    - Under this case, what is the training and test score? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC \n",
    "from sklearn.svm import LinearSVC\n",
    "lr_svc = LinearSVC(random_state = 0) # C = 1 as default\n",
    "lr_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9225352112676056, 0.965034965034965)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_svc.score(X_train, y_train), lr_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU available\n",
    "import os\n",
    "n_cpu = os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LinearSVC(random_state=22), n_jobs=2,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 100000]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVC with GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define Function\n",
    "linear_svc = LinearSVC(random_state = 22) # must specify random state here\n",
    "\n",
    "# Define a list of hyperparameters\n",
    "params_svc = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 100000]   }\n",
    "\n",
    "# a large C approximates hard margin SVM scenario\n",
    "\n",
    "grid_lrsvc = GridSearchCV(linear_svc, params_svc, n_jobs = 2)\n",
    "\n",
    "grid_lrsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lrsvc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178403755868545"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lrsvc.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9178403755868545, 0.9300699300699301)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param_svc= LinearSVC(random_state = 22, C= 0.001)\n",
    "best_param_svc.fit(X_train, y_train)\n",
    "best_param_svc.score(X_train,y_train), best_param_svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with Kernel Functions\n",
    "In most cases, SVM comes together with kernel functions, so that the classifier can handle non-linear separable cases. For implementation, we use syntax:\n",
    "**<center>sklearn.svm.SVC()</center>**\n",
    "- First, be aware that the last three letters, SVC, are capitalized.\n",
    "- **C:** used to specify how acceptable for margin violations. Same as linear case.\n",
    "- **kernel:** used to specify the kernel function. Choose from {'linear', 'poly', 'rbf', 'sigmoid', 'recomputed'}. These are different kernel functions. We introduced only two of them (i.e., poly and rbf). The default value is 'rbf'.\n",
    "- **degree:** used when kernel='poly', to specify the polynomial degree. Default value = 3.\n",
    "- **gamma:** used when kernel = 'rbf', we can set it manually. Default is relevant to feature number and feature variations.\n",
    "- Set random state for technical reasons.\n",
    "\n",
    "To explore other details, check https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "#### Practice\n",
    "- Use the same cancer data and training-test splittion. Train an SVM model, let kernel function be rbf, leave other parameters and hyperparameters as default. What is the accuracy for training set and test set?\n",
    "- Train an SVM model, let kernel function be rbf. \n",
    "    - Apply grid search with 5-fold CV. Let choices of C be: [0.001, 0.01, 0.1, 1, 10, 100, 10000]. \n",
    "    - Let choices of gamma be: [0.0001, 0.001,0.001,0.1,1,10]. \n",
    "    - Which model is the best? \n",
    "    - What is the performance of the training and test set?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_base = SVC(random_state = 0, kernel = 'rbf')\n",
    "svm_base.fit(X_train, y_train)\n",
    "svm_base.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=0), n_jobs=2,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 10000],\n",
       "                         'gamma': [0.0001, 0.001, 0.001, 0.1, 1, 10]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Define Function\n",
    "svc = SVC(random_state = 0, kernel = 'rbf')\n",
    "\n",
    "#define a list of parameters\n",
    "param_svc_kernel = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 10000]  ,\n",
    "                    'gamma': [0.0001, 0.001,0.001,0.1,1,10]     } # C = 10,000 mimics hard-margin SVM\n",
    "\n",
    "#apply grid search\n",
    "grid_svc = GridSearchCV(svc, param_svc_kernel, cv = 5, n_jobs=2)\n",
    "\n",
    "grid_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.0001}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.958041958041958"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be implemented in several ways. We discuss two specific cases in today's class: (1) If all predictors are categorical, and (2) If some or all predictors are continuous.\n",
    "\n",
    "#### Categorical Features\n",
    "If all predictors are categorical, use syntax:\n",
    "\n",
    "**<center>sklearn.naive_bayes.CategoricalNB()</center>**\n",
    "- alpha: A smoothing factor. The default value is 1. To get the same result as manually calculated, set alpha = 0.\n",
    "\n",
    "#### Continuous Features\n",
    "If all predictors are continuous, we would need to use one of the methods below:\n",
    "1. Bin the continuous variable first, then use the previously mentioned syntax, sklearn.naive_bayes.CategoricalNB(). *We do not discuss on this method in here.*\n",
    "\n",
    "2. Assume the data follows a normal distribution. Then we can use the syntax below: \n",
    "\n",
    "**<center>sklearn.naive_bayes.GaussianNB()</center>**\n",
    "\n",
    "For both models (i.e., categorical NB and Gaussian NB), we can obtain predicted probability for each class using .predict_proba. \n",
    "\n",
    "#### Practice 1\n",
    "Replicate the result of the in-class practice (i.e., firm report example, probability = 0.47).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in data: \n",
    "## Charges: =1 if yes, =0 if no.\n",
    "## Size: =1 if large, =0 if small\n",
    "## Y: =1 if T, =0 if F\n",
    "X = np.array([[1,0], [0,0], [0,1], [0, 1], [0,0], [0,0], [1,0], [1,1], [0,1], [1,1]])\n",
    "Y = np.array([1,1,1,1,1,1,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y label predicted is: [0]\n",
      "P(Y=T|Yes, Small) is: 0.4705882353003458\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "cat_nb = CategoricalNB(alpha = 0) # alpha = 1 being default, cannot replicate\n",
    "cat_nb.fit(X, Y) # no splitting to keep consistent with class\n",
    "\n",
    "# New Record: Yes, Small\n",
    "X_new = [[1, 0]]\n",
    "print(\"Y label predicted is:\", cat_nb.predict(X_new))\n",
    "print(\"P(Y=T|Yes, Small) is:\", cat_nb.predict_proba(X_new)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice 2\n",
    "Using the iris data, train a Naive Bayes model. Assume variables are normally distributed.\n",
    "- Split the data into training and test\n",
    "- Train the model on the training set (Use Gaussian NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() \n",
    "# load the complete data information in. It consists both data and descriptive info.\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "g_nb = GaussianNB()\n",
    "g_nb.fit(X_train, y_train)\n",
    "g_nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 2, 1, 1, 1, 2, 1, 0, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 0, 2,\n",
       "       0, 1, 2, 0, 2, 2, 2, 2, 0, 0, 1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+000, 1.46393987e-015, 1.72483299e-025],\n",
       "        [6.55179468e-208, 6.29854854e-006, 9.99993701e-001],\n",
       "        [3.33015554e-109, 9.52875628e-001, 4.71243721e-002],\n",
       "        [1.15489365e-155, 6.85968450e-002, 9.31403155e-001],\n",
       "        [1.01000872e-105, 9.84267197e-001, 1.57328025e-002],\n",
       "        [6.29860069e-113, 9.20181687e-001, 7.98183131e-002],\n",
       "        [1.08778224e-113, 8.32119386e-001, 1.67880614e-001],\n",
       "        [1.27058863e-136, 2.63710210e-001, 7.36289790e-001],\n",
       "        [4.13832799e-059, 9.99973268e-001, 2.67324406e-005],\n",
       "        [1.00000000e+000, 3.84469487e-016, 2.34514599e-026],\n",
       "        [4.68558639e-232, 2.41953265e-007, 9.99999758e-001],\n",
       "        [7.83223625e-074, 9.99832501e-001, 1.67498518e-004],\n",
       "        [1.21332125e-221, 5.43910382e-006, 9.99994561e-001],\n",
       "        [9.15823459e-278, 3.65781135e-010, 1.00000000e+000],\n",
       "        [9.99999984e-001, 1.62617229e-008, 6.70154060e-018],\n",
       "        [1.24977087e-250, 3.57862227e-008, 9.99999964e-001],\n",
       "        [3.32661557e-059, 9.99994816e-001, 5.18364458e-006],\n",
       "        [8.58820253e-076, 9.99814554e-001, 1.85446168e-004],\n",
       "        [1.14820897e-125, 9.84717574e-001, 1.52824263e-002],\n",
       "        [3.28491789e-079, 9.99648873e-001, 3.51127382e-004],\n",
       "        [1.00000000e+000, 8.11439188e-016, 8.06315989e-026],\n",
       "        [1.31476385e-200, 6.12657136e-006, 9.99993873e-001],\n",
       "        [1.00000000e+000, 1.19341874e-015, 1.26071212e-025],\n",
       "        [1.00885156e-067, 9.99967748e-001, 3.22522039e-005],\n",
       "        [1.57873708e-158, 5.01192745e-002, 9.49880725e-001],\n",
       "        [1.00000000e+000, 2.84544586e-016, 1.95226374e-026],\n",
       "        [6.65287037e-179, 1.74114397e-003, 9.98258856e-001],\n",
       "        [8.33386395e-175, 1.22028823e-003, 9.98779712e-001],\n",
       "        [4.26788908e-211, 9.71670741e-006, 9.99990283e-001],\n",
       "        [1.23146440e-137, 2.72450084e-001, 7.27549916e-001],\n",
       "        [1.00000000e+000, 1.56060785e-014, 1.95594489e-024],\n",
       "        [1.00000000e+000, 2.70375912e-012, 6.17363749e-022],\n",
       "        [2.72605060e-072, 9.99959909e-001, 4.00906937e-005],\n",
       "        [1.41857689e-081, 9.99871524e-001, 1.28476152e-004],\n",
       "        [7.12441496e-091, 9.98920269e-001, 1.07973139e-003],\n",
       "        [1.00000000e+000, 1.97042595e-012, 4.31783693e-022],\n",
       "        [1.00000000e+000, 2.28770862e-015, 2.86883598e-025],\n",
       "        [1.00000000e+000, 2.88560394e-017, 2.12161569e-027]]),\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_nb.predict_proba(X_test), iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
